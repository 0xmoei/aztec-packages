#!/bin/bash
# Use ci3 script base.
source $(git rev-parse --show-toplevel)/ci3/base/source

# Tests the various modes of bootstrap build usage:
# - CI=1 or CI=0
# - bootstrap.sh full or bootstrap.sh fast
# - Use TEST=0

function delete_test_cache() {
  AWS_SECRET_ACCESS_KEY=minioadmin AWS_ACCESS_KEY_ID=minioadmin \
    aws --endpoint http://localhost:12000 \
      s3 rm s3://aztec-ci-artifacts --recursive --exclude "*" --include "build-cache/barretenberg-test-cache-*.tar.gz" 2>&1 || true
}

function start_minio() {
  if nc -z 127.0.0.1 12000 >/dev/null 2>&1; then
    # MinIO is already running.
    return
  fi
  echo "Starting MinIO..."
  docker run -d --name minio \
    -p 12000:9000 -p 12001:9001 \
    -v minio-data:/data \
    quay.io/minio/minio server /data --console-address ":12001"

  # Wait for MinIO to be ready
  while ! nc -z 127.0.0.1 12000 >/dev/null 2>&1; do
    sleep 1
  done

  # Create the cache bucket
  echo "Creating MinIO bucket for cache..."
  AWS_ACCESS_KEY_ID="minioadmin" AWS_SECRET_ACCESS_KEY="minioadmin" \
    aws --endpoint-url http://localhost:12000 s3 mb s3://aztec-ci-artifacts 2>/dev/null || true
}
export S3_BUILD_CACHE_AWS_PARAMS="--endpoint-url http://localhost:12000"
export AWS_SECRET_ACCESS_KEY=minioadmin
export AWS_ACCESS_KEY_ID=minioadmin

start_minio
# if we have minio already running make sure the cache is deleted
# otherwise it will be run by next earthly local command
nc -z 127.0.0.1 12000 2>/dev/null >/dev/null && delete_test_cache
# Our assertions (thanks to -e flag):
# expect file to not exist at first
if minio_cache_only ../../barretenberg/cpp/+test-cache-read 2>/dev/null ; then
  echo "Cache read without write should fail!"
  exit 1
fi
minio_cache_only ../../barretenberg/cpp/+test-cache-write 2>/dev/null
[ -d ~/.minio/data/*/*/barretenberg-test-cache*.tar.gz ] # minio cache files should be written
minio_cache_only ../../barretenberg/cpp/+test-cache-read 2>/dev/null # we should be able to read now
echo "Success!"
